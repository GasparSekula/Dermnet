{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dermnet - Kamień milowy 3 (Modelowanie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pakiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Przygotowanie danych\n",
    "### 1.1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15557 entries, 0 to 15556\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   image   15557 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 121.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_pickle('train_data_1.pkl')\n",
    "df2 = pd.read_pickle('train_data_2.pkl')\n",
    "X = pd.concat([df1, df2], axis=0)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmiana rozdzielczości z 224x224 px na 75x75 px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize all images\n",
    "X['image'] = X['image'].apply(lambda x: cv2.resize(x, (75, 75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Rotacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_all_images(data_frame, colname = 'image'):\n",
    "    images_90 = []\n",
    "    for image in data_frame[colname]:\n",
    "        image_90 = np.rot90(image)\n",
    "        images_90.append(image_90)\n",
    "    return pd.DataFrame({colname: images_90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rotated_90 = rotate_all_images(X)\n",
    "X_rotated_180 = rotate_all_images(X_rotated_90)\n",
    "X_rotated_270 = rotate_all_images(X_rotated_180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Modyfikacje obrazów\n",
    "#### Funkcje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2137)\n",
    "chosen_indexes = random.sample(range(len(X['image'])), 8)\n",
    "\n",
    "def visualize_random_images(df, indexes = [0, 1, 2, 3, 4, 5, 6, 7]):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    for i in range(8):\n",
    "        img = df['image'].iloc[indexes[i]]\n",
    "        ax = axes[i//4, i%4]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "\n",
    "def convert_to_grayscale(data_frame, colname='image'):\n",
    "    '''\n",
    "    Convert the RGB images to grayscale\n",
    "    '''\n",
    "    result = []\n",
    "    k=0\n",
    "    for image in data_frame['image']:\n",
    "        r, g, b = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "        gray = 0.33 * r + 0.33 * g + 0.33 * b\n",
    "        result.append(gray)\n",
    "    result = pd.DataFrame({colname : result})\n",
    "    return result\n",
    "\n",
    "def convert_to_grayscale_one_color(data_frame, colname='image'):\n",
    "    '''\n",
    "    Convert the RGB images to grayscale\n",
    "    '''\n",
    "    result = []\n",
    "    k=0\n",
    "    for image in data_frame['image']:\n",
    "        r, g, b = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "        gray = r + g + b\n",
    "        result.append(gray)\n",
    "    result = pd.DataFrame({colname : result})\n",
    "    return result\n",
    "\n",
    "\n",
    "def visualize_random_images_grey(df, indexes = [0, 1, 2, 3, 4, 5, 6, 7]):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    for i in range(8):\n",
    "        img = df['image'].iloc[indexes[i]]\n",
    "        ax = axes[i//4, i%4]\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "\n",
    "def extract_rgb_channel(data_frame, colname='image', color='red'):\n",
    "    '''\n",
    "    Convert the RGB images to grayscale\n",
    "    '''\n",
    "    if color == 'red':\n",
    "        channel = 0\n",
    "    elif color == 'green':\n",
    "        channel = 1\n",
    "    elif color == 'blue':\n",
    "        channel = 2\n",
    "    result = []\n",
    "    k=0\n",
    "    for image in data_frame['image']:\n",
    "        modified_image = np.zeros_like(image)\n",
    "        modified_image[:,:,channel] = image[:,:,channel]\n",
    "        result.append(modified_image)\n",
    "    result = pd.DataFrame({colname : result})\n",
    "    return result\n",
    "\n",
    "\n",
    "def global_threshold(df, tresh, colname = 'image'):\n",
    "    '''\n",
    "    Runs threshold on all images in df\n",
    "    '''\n",
    "    assert df[colname].iloc[0].ndim == 2, \"images must be in greyscale\"\n",
    "    result = []\n",
    "    for image in df[colname]:\n",
    "        result.append((image > tresh) * 255)\n",
    "    result = pd.DataFrame({colname : result})\n",
    "    return result\n",
    "\n",
    "def reverse_threshold(df, tresh, colname = 'image'):\n",
    "    '''\n",
    "    Runs threshold on all images in df\n",
    "    '''\n",
    "    assert df[colname].iloc[0].ndim == 2, \"images must be in greyscale\"\n",
    "    result = []\n",
    "    for image in df[colname]:\n",
    "        result.append((image < tresh) * 255)\n",
    "    result = pd.DataFrame({colname : result})\n",
    "    return result\n",
    "\n",
    "def detect_images(df, low_th = 50, high_th = 150, blur_ksize = 5, colname = 'image'):\n",
    "    '''\n",
    "    Runs edge detection on all images in df\n",
    "    '''\n",
    "    assert df[colname].iloc[0].ndim == 2, \"images must be in greyscale\"\n",
    "    result = []\n",
    "    for image in df[colname]:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        blurred_image = cv2.GaussianBlur(image, (blur_ksize, blur_ksize), 0)\n",
    "\n",
    "        edges = cv2.Canny(blurred_image, low_th, high_th)  \n",
    "        result.append(edges)\n",
    "    result = pd.DataFrame({colname : result})\n",
    "    return result\n",
    "\n",
    "def convert_to_negative_image(df, colname = 'image'):\n",
    "    '''\n",
    "    Converts the image to negative\n",
    "    '''\n",
    "    result = []\n",
    "    for image in df[colname]:\n",
    "        result.append(255 - image)\n",
    "    result = pd.DataFrame({colname : result})\n",
    "    return result\n",
    "\n",
    "def erosion(df, kernel_size=(5,5), iterations=1, colname='image'):\n",
    "    result = []\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    for image in df[colname]:\n",
    "        eroded_image = cv2.erode(image, kernel, iterations=iterations)\n",
    "        result.append(eroded_image)\n",
    "    result = pd.DataFrame({colname: result})\n",
    "    return result\n",
    "\n",
    "def dilation(df, kernel_size=(5,5), iterations=1, colname='image'):\n",
    "    result = []\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    for image in df[colname]:\n",
    "        dilated_image = cv2.dilate(image, kernel, iterations=iterations)\n",
    "        result.append(dilated_image)\n",
    "    result = pd.DataFrame({colname: result})\n",
    "    return result\n",
    "\n",
    "def reverse_hough_transform(df, threshold=100, colname='image'):\n",
    "    assert df[colname].iloc[0].ndim == 2, \"images must be in greyscale\"\n",
    "    result = []\n",
    "    for image in df[colname]:\n",
    "        image = image.astype(np.uint8)  # Convert to CV_8U data type\n",
    "        edges = cv2.Canny(image, 50, 150, apertureSize=3)\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold)  # Define 'lines' variable\n",
    "        if lines is not None:\n",
    "            for rho, theta in lines[:, 0]:\n",
    "                a = np.cos(theta)\n",
    "                b = np.sin(theta)\n",
    "                x0 = a * rho\n",
    "                y0 = b * rho\n",
    "                x1 = int(x0 + 1000 * (-b))\n",
    "                y1 = int(y0 + 1000 * (a))\n",
    "                x2 = int(x0 - 1000 * (-b))\n",
    "                y2 = int(y0 - 1000 * (a))\n",
    "                cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        result.append(image)\n",
    "    result = pd.DataFrame({colname: result})\n",
    "    return result\n",
    "\n",
    "def flatten_images(df, colname='image', batch_size=100):\n",
    "    num_images = len(df)\n",
    "    flattened_data = []\n",
    "    for start in range(0, num_images, batch_size):\n",
    "        end = min(start + batch_size, num_images)\n",
    "        batch_images = df[colname].iloc[start:end]\n",
    "        batch_flattened = []\n",
    "        for image in batch_images:\n",
    "            image_array = img_to_array(image)\n",
    "            batch_flattened.append(image_array.flatten() / 255.0)\n",
    "        batch_flattened = np.array(batch_flattened)\n",
    "        flattened_data.append(batch_flattened)\n",
    "    flattened_data = np.concatenate(flattened_data, axis=0)\n",
    "    flattened_df = pd.DataFrame(flattened_data, columns=[f'{colname}_{i}' for i in range(flattened_data.shape[1])])\n",
    "    return flattened_df\n",
    "\n",
    "\n",
    "def modify_pictures(df, colname='image'):\n",
    "\n",
    "    # convert and flatten\n",
    "    grayscale = convert_to_grayscale(df, colname)\n",
    "    grayscale_flatten = flatten_images(grayscale, colname)\n",
    "    print('grayscale done')\n",
    "    red = extract_rgb_channel(df, colname, 'red')\n",
    "    red_flatten = flatten_images(red, colname)\n",
    "    print('red done')\n",
    "    green = extract_rgb_channel(df, colname, 'green')\n",
    "    green_flatten = flatten_images(green, colname)\n",
    "    print('green done')\n",
    "    blue = extract_rgb_channel(df, colname, 'blue')\n",
    "    blue_flatten = flatten_images(blue, colname)\n",
    "    print('blue done')\n",
    "    thresholded = global_threshold(grayscale, 100, colname)\n",
    "    thresholded_flatten = flatten_images(thresholded, colname)\n",
    "    print('threshold done') \n",
    "    eroded = erosion(df, colname=colname)\n",
    "    eroded_flatten = flatten_images(eroded, colname)\n",
    "    print('eroded done')\n",
    "    dilated = dilation(df, colname=colname)\n",
    "    dilated_flatten = flatten_images(dilated, colname)\n",
    "    print('dilated done')\n",
    "    hough = reverse_hough_transform(grayscale, colname=colname)\n",
    "    hough_flatten = flatten_images(hough, colname)\n",
    "    print('hough done')\n",
    "\n",
    "    # concatenate\n",
    "    result = pd.concat([grayscale_flatten, red_flatten, green_flatten, blue_flatten, thresholded_flatten, eroded_flatten, dilated_flatten, hough_flatten], axis=1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Ramka danych `X`\n",
    "Obrazy z wyciągniętymi kanałami R, G i B niosą w sumie te same informacje, co obraz kolorowy czy negatyw. Nie będziemy więc tych dwóch modyfikacji uwzględniać. Dzięki temu zmniejszamy liczbę 'kolumn' w docelowej ramce danych o 40%, niosąc te same informacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grayscale done\n",
      "red done\n",
      "green done\n",
      "blue done\n",
      "threshold done\n",
      "eroded done\n",
      "dilated done\n",
      "hough done\n"
     ]
    }
   ],
   "source": [
    "# X_processed = modify_pictures(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Ramka obrazów obróconych `X_rotated_alpha`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grayscale done\n",
      "red done\n",
      "green done\n",
      "blue done\n",
      "threshold done\n",
      "eroded done\n",
      "dilated done\n",
      "hough done\n",
      "90 done\n",
      "grayscale done\n",
      "red done\n",
      "green done\n",
      "blue done\n",
      "threshold done\n",
      "eroded done\n",
      "dilated done\n",
      "hough done\n",
      "180 done\n",
      "grayscale done\n",
      "red done\n",
      "green done\n",
      "blue done\n",
      "threshold done\n",
      "eroded done\n",
      "dilated done\n",
      "hough done\n"
     ]
    }
   ],
   "source": [
    "X_rot_90_processed = modify_pictures(X_rotated_90)\n",
    "print('90 done')\n",
    "X_rot_180_processed = modify_pictures(X_rotated_180)\n",
    "print('180 done')\n",
    "X_rot_270_processed = modify_pictures(X_rotated_270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. PCA na `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components = 0.9)\n",
    "# X_pca = pca.fit_transform(X_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15557, 239)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pca_df = pd.DataFrame(X_pca)\n",
    "# X_pca_df.to_csv('X_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. PCA na `X_rotated_alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Programy\\Anaconda\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert pandas DataFrames to Dask DataFrames\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dX_processed \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(X_processed, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m dX_rot_90_processed \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(X_rot_90_processed, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      6\u001b[0m dX_rot_180_processed \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(X_rot_180_processed, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      7\u001b[0m dX_rot_270_processed \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(X_rot_270_processed, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Programy\\Anaconda\\Lib\\site-packages\\dask\\dataframe\\io\\io.py:312\u001b[0m, in \u001b[0;36mfrom_pandas\u001b[1;34m(data, npartitions, chunksize, sort, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m     divisions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(locations)\n\u001b[0;32m    308\u001b[0m dsk \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    309\u001b[0m     (name, i): data\u001b[38;5;241m.\u001b[39miloc[start:stop]\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (start, stop) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(locations[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], locations[\u001b[38;5;241m1\u001b[39m:]))\n\u001b[0;32m    311\u001b[0m }\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_dd_object(dsk, name, data, divisions)\n",
      "File \u001b[1;32mc:\\Programy\\Anaconda\\Lib\\site-packages\\dask\\dataframe\\core.py:8365\u001b[0m, in \u001b[0;36mnew_dd_object\u001b[1;34m(dsk, name, meta, divisions, parent_meta)\u001b[0m\n\u001b[0;32m   8360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generic constructor for dask.dataframe objects.\u001b[39;00m\n\u001b[0;32m   8361\u001b[0m \n\u001b[0;32m   8362\u001b[0m \u001b[38;5;124;03mDecides the appropriate output class based on the type of `meta` provided.\u001b[39;00m\n\u001b[0;32m   8363\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_parallel_type(meta):\n\u001b[1;32m-> 8365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_parallel_type(meta)(dsk, name, meta, divisions)\n\u001b[0;32m   8366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_arraylike(meta) \u001b[38;5;129;01mand\u001b[39;00m meta\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m   8367\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mda\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Programy\\Anaconda\\Lib\\site-packages\\dask\\dataframe\\core.py:4929\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, dsk, name, meta, divisions)\u001b[0m\n\u001b[0;32m   4922\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dsk, name, meta, divisions)\n\u001b[0;32m   4923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   4925\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpartitions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions,\n\u001b[0;32m   4926\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns],\n\u001b[0;32m   4927\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: typename(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)),\n\u001b[0;32m   4928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: typename(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta)),\n\u001b[1;32m-> 4929\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_dtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   4930\u001b[0m             col: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta[col]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m   4931\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta[col], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4932\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4933\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4934\u001b[0m         },\n\u001b[0;32m   4935\u001b[0m     }\n\u001b[0;32m   4936\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   4938\u001b[0m         {\n\u001b[0;32m   4939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpartitions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4949\u001b[0m         }\n\u001b[0;32m   4950\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Programy\\Anaconda\\Lib\\site-packages\\dask\\dataframe\\core.py:4931\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   4922\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dsk, name, meta, divisions)\n\u001b[0;32m   4923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   4925\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpartitions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions,\n\u001b[0;32m   4926\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns],\n\u001b[0;32m   4927\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: typename(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)),\n\u001b[0;32m   4928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: typename(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta)),\n\u001b[0;32m   4929\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_dtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   4930\u001b[0m             col: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta[col]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m-> 4931\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta[col], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4932\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4933\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4934\u001b[0m         },\n\u001b[0;32m   4935\u001b[0m     }\n\u001b[0;32m   4936\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdask\u001b[38;5;241m.\u001b[39mlayers[name]\u001b[38;5;241m.\u001b[39mcollection_annotations\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   4938\u001b[0m         {\n\u001b[0;32m   4939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpartitions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpartitions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4949\u001b[0m         }\n\u001b[0;32m   4950\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Programy\\Anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Programy\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m casted_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_indexer(key)\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Convert pandas DataFrames to Dask DataFrames\n",
    "dX_processed = dd.from_pandas(X_processed, npartitions=10)\n",
    "dX_rot_90_processed = dd.from_pandas(X_rot_90_processed, npartitions=10)\n",
    "dX_rot_180_processed = dd.from_pandas(X_rot_180_processed, npartitions=10)\n",
    "dX_rot_270_processed = dd.from_pandas(X_rot_270_processed, npartitions=10)\n",
    "\n",
    "# Concatenate Dask DataFrames\n",
    "dX_rot_processed = dd.concat([dX_processed, dX_rot_90_processed, dX_rot_180_processed, dX_rot_270_processed], axis=0)\n",
    "\n",
    "# Compute the final result\n",
    "X_rot_processed = dX_rot_processed.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
